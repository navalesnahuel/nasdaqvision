# =============================================================================
# Docker Compose Configuration File
# Maintainer: Nahuel Navales
# Proyect: Platform
# =============================================================================

x-spark-common: &spark-common
  build:
    context: .
    dockerfile: Dockerfiles/spark.Dockerfile
  networks:
    - admin

x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: Dockerfiles/airflow.Dockerfile
  env_file:
    - Dockerfiles/conf/conf-airflow/.env
  volumes:
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/jobs 
    - shared_volume:/opt/shared_data
  networks:
    - admin
  depends_on:
    - postgres

services:
  spark-master:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    container_name: sparkmaster
    volumes:
      - ./:/opt/spark_project
    restart: always

  spark-worker-1:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    env_file:
      - Dockerfiles/conf/conf-spark/.env
    container_name: sparkworker
    restart: always

  mysql:
    container_name: mysql-metastore
    image: mysql:8.0
    env_file:
      - Dockerfiles/conf/conf-mysql/.env
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - admin
    restart: always

  postgres:
    container_name: postgresql
    image: postgres:14.0
    env_file:
      - Dockerfiles/conf/conf-postgres/.env
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - admin
    restart: always

  minio:
    container_name: minio
    image: "minio/minio:latest"
    volumes:
      - minio_data:/data
    ports:
      - 9000:9000
      - 9001:9001
    env_file:
      - Dockerfiles/conf/conf-minio/.env
    command: server /data --console-address ":9001"
    networks:
      - admin
    restart: always

  hive-metastore:
    container_name: hive-metastore
    build:
      context: .
      dockerfile: Dockerfiles/hive.Dockerfile
    env_file:
      - Dockerfiles/conf/conf-hive/.env
    ports:
      - "9083:9083"
    depends_on:
      - mysql
    networks:
      - admin
    restart: always

  spark-thrift-server:
    container_name: spark-thrift-server
    build:
      context: .
      dockerfile: Dockerfiles/spark.Dockerfile
    ports:
      - "12000:12000"
    environment:
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
    command: /opt/bitnami/spark/sbin/start-thriftserver.sh \
      --master spark://spark-master:7077 \
      --conf spark.sql.catalogImplementation=hive \
      --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 \
      --executor-cores 2 \
      --executor-memory 2G
    networks:
      - admin
    restart: always

  streamlit:
    container_name: streamlit
    build:
      context: .
      dockerfile: Dockerfiles/streamlit.Dockerfile
    ports:
      - "8501:8501"
    restart: always
    networks:
      - admin

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 8080:8080
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    restart: always
    command: bash -c "airflow db migrate && airflow users create --username admin --firstname Nahuel --lastname Navales --role Admin --email navalesnahuel@gmail.com --password admin && airflow scheduler"
      
volumes:
  minio_data:
  mysql_data:
  postgres_data:
  spark_data:
  shared_volume:

networks:
  admin:
    driver: bridge
